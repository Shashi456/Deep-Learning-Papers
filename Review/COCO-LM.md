# COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining
- COCO-LM, a new self-supervised learning framework that pretrains Language Models by Correcting challenging errors and Contrasting text sequences. 
  - COCO-LM employs an auxiliary language model to mask-and-predict tokens in original text sequences.
- Earlier PLM Challenges include
  - Randomly altered texts contain many non-information signals no longer useful after a certain amount of pretraining.
  - Pretraining at token level does not explicitly learn language semantics at sentence level.
- 
