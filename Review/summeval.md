# Summ-Eval: Re-evaluating Summarization Evaluation

- This paper is a meta-evaluation (evaluating evaluation) for metrics in summarization.
- The contributions of the paper include
  - Re-evaluate 12 evaluation metrics with expert & crowd-sourced metrics on 23 neural summarization models.
  - Release Largest collection of summaries generated by models trained on CNN/DailyMail 
  - Share a toolkit for evaluating summarization models across a broad range of automated metrics
  - Share the largest and most diverse (in terms of model types) collection of human judgements, of model 
  generated summaries by both experts and crowd source workers.
- 
  
